# -*- coding: utf-8 -*-
"""MSA+resnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Utxt5GfhXUgTZ6i-cnPEkBCas4owXP27
"""

!pip install einops
!pip install timm

import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import timm
 
from torch import nn
from torch import Tensor
from torchvision.transforms import Compose, Resize, ToTensor, Normalize
from einops import rearrange, reduce, repeat
from einops.layers.torch import Rearrange, Reduce
from torchsummary import summary

from functools import partial

# 데이터 로드
from torchvision import datasets, transforms
import timm.data.transforms_factory as tff

transforms = transforms.Compose([transforms.ToTensor(),
                                 transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
                                 transforms.RandomHorizontalFlip(0.5),
                                 transforms.RandomVerticalFlip(0.0),
                                 transforms.ColorJitter(0.0),
                                 ])

training_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms)
validation_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms)

training_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=96, shuffle=True, num_workers = 4)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=96, shuffle=False, num_workers = 4)

classes = ("plane", "car", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck")

print("trainset 사이즈 : ", len(training_loader.dataset),
"\ntestset 사이즈 : ",len(validation_loader.dataset))

# 이미지 시각화
import numpy as np

def im_convert(tensor):
  image = tensor.clone().detach().numpy()
  image = image.transpose(1, 2, 0)
  image = image * np.array([0.5, 0.5, 0.5] + np.array([0.5, 0.5, 0.5]))
  image = image.clip(0, 1)
  return image

dataiter = iter(training_loader)
images, labels = dataiter.next()
fig = plt.figure(figsize=(10, 4))

for i in np.arange(10):
  # row 2 column 10
  ax = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])
  plt.imshow(im_convert(images[i]))
  ax.set_title(classes[labels[i].item()])

class BasicBlock(nn.Module): # resnet 18/ 34를 위한 block
    expansion = 1

    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()

        #residual function
        self.residual_function = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels * BasicBlock.expansion)
        )

        #shortcut
        self.shortcut = nn.Sequential()

        #the shortcut output dimension is not the same with residual function
        #use 1*1 convolution to match the dimension
        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels * BasicBlock.expansion)
            )

    def forward(self, x):
        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))

class BottleNeck(nn.Module): # resnet-50 이상을 위한 block    
    expansion = 4
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.residual_function = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels * BottleNeck.expansion),
        )

        self.shortcut = nn.Sequential()

        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),
                nn.BatchNorm2d(out_channels * BottleNeck.expansion)
            )

    def forward(self, x):
        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))

class Patchembedding(nn.Module):
  def __init__(self, in_channels: int=3, patch_size:int = 2, emb_dim: int=12, img_size: int = 32):
    self.patch_size = patch_size # patch_size는 바꾸어줄 수 있도록 따로 선언 한줄
    super().__init__()
    # image -> (3,224,224)짜리 이미지 -> (196, 768)로 임베딩 
    self.projection = nn.Sequential(
        nn.Conv2d(in_channels, emb_dim, kernel_size = patch_size, stride = patch_size),
        Rearrange("b d h w -> b (h w) d")
        ) # output = [batch, patch_num, emb_dim] 형태
    self.cls_token = nn.Parameter(torch.randn(1,1, emb_dim))
    # 197 -> 768로 변환한 학습가능한 1D position// cos, sin positional encoding으로 바꿔보자
    self.position = nn.Parameter(torch.randn((img_size//patch_size)**2+1, emb_dim))  
    #self.cos_position = nn.



  def forward(self, x:Tensor) -> Tensor:
    b = x.shape[0]
    proj_x = self.projection(x)
    cls_token = repeat(self.cls_token, '() c d -> b c d', b = b)
    proj_x = torch.concat([cls_token, proj_x], dim=1)
    position = self.position
    proj_x = proj_x + position
    return proj_x

class MultiHeadAttention(nn.Module):
  def __init__(self, emb_dim = 16*512, num_heads = 4, dropout:float=0.):
    self.emb_dim = emb_dim
    self.num_heads = num_heads
    super().__init__()
    self.querry = nn.Linear(emb_dim, emb_dim)
    self.key = nn.Linear(emb_dim, emb_dim)
    self.value = nn.Linear(emb_dim, emb_dim)
    self.projection = nn.Linear(emb_dim, emb_dim)
    self.drop = nn.Dropout(dropout)

# dropout/ Mask 사용 연구

  def forward(self, token: Tensor) -> Tensor:
    queries = rearrange(self.querry(token), "b n (h d) -> b h n d", h = self.num_heads)
    keys = rearrange(self.key(token), "b n (h d) -> b h n d", h = self.num_heads)
    values  = rearrange(self.value(token), "b n (h d) -> b h n d", h = self.num_heads)
    energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)

    attention = F.softmax(energy, dim=-1)/(self.emb_dim**(1/2))
    #attention = self.drop(attention)
    att_score = torch.einsum('bhqk, bhvd -> bhqd', attention, values)
    result = rearrange(att_score, 'b h n d -> b n (h d)')
    result = self.projection(result) # MLP층 

    return result

class ResidualAdd(nn.Module):
    def __init__(self, fn):
        super().__init__()
        self.fn = fn
        
    def forward(self, x, **kwargs):
        res = x
        x = self.fn(x, **kwargs)
        x += res
        return x

class FeedForwardBlock(nn.Sequential):
    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):
        super().__init__(
            nn.Linear(emb_size, expansion * emb_size),
            nn.GELU(),
            nn.Dropout(drop_p),
            nn.Linear(expansion * emb_size, emb_size),
        )

class TransformerBlock(nn.Sequential):
    def __init__(self,
                 emb_size: int = 16*512,
                 dropout: float = 0.,
                 forward_expansion: int = 4,
                 forward_dropout: float = 0.,
                 ** kwargs):
        super().__init__(
            ResidualAdd(nn.Sequential(
                nn.LayerNorm(emb_size),
                MultiHeadAttention(emb_size, **kwargs),
                nn.Dropout(dropout)
            )),
            ResidualAdd(nn.Sequential(
                nn.LayerNorm(emb_size),
                FeedForwardBlock(
                    emb_size, expansion=forward_expansion, drop_p=forward_dropout),
                nn.Dropout(dropout)
            )
            ))

class TransformerEncoder(nn.Sequential):
    def __init__(self, 
                 depth: int = 1, 
                 emb_dim: int = 12,
                 dropout: float = 0.,
                 forward_expansion: int = 4,
                 forward_dropout: float = 0.,
                 ** kwargs):
        super().__init__(*[TransformerBlock(emb_dim, dropout, forward_expansion, forward_dropout, **kwargs) for _ in range(depth)])

class Classifier(nn.Sequential):
    def __init__(self, emb_size: int = 512, n_classes: int = 10):
        super().__init__(
            Reduce('b n e -> b e', reduction='mean'),
            nn.LayerNorm(emb_size), 
            nn.Linear(emb_size, n_classes))

"""# 처음에 MSA block추가"""

class AlterNet2(nn.Module):

    def __init__(self, block, num_block, num_classes=10):
        super().__init__()

        self.to_token = Patchembedding(3, 2, 12, 32)
        self.transformers = TransformerEncoder(4, 12, 0., 4, 0.)
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(12, 64, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True))

        self.in_channels = 64
        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)
        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)
        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)
        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)
        
      
    def _make_layer(self, block, out_channels, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_channels, out_channels, stride))
            self.in_channels = out_channels * block.expansion

        return nn.Sequential(*layers)

    def forward(self, x):
        output = self.to_token(x)
        output = self.transformers(output)
        # cls_ = output[:,0,:]
        # cls_ = repeat(cls_, 'b d -> b n d', n = 256) 
        output = output[:,1:, :] # 해봤다 안해봤다 해보기
        output = rearrange(output, "b (h w) d -> b d h w", h=16)
        output = self.conv1(output)
        output = self.conv2_x(output)
        output = self.conv3_x(output)
        output = self.conv4_x(output)
        output = self.conv5_x(output)
        output = self.avg_pool(output)
        output = output.view(output.size(0), -1)
        output = self.fc(output)

        return output

ex_tensor = torch.randn([8,3,32,32])
model = AlterNet2(BasicBlock, [1,1,1,1])
model(ex_tensor).shape

import torch.optim as optim

USE_CUDA = torch.cuda.is_available()
DEVICE = torch.device("cuda" if USE_CUDA else "cpu")
model = AlterNet2(BasicBlock, [2,2,2,2]).to(DEVICE)
criterion = F.cross_entropy
optimizer = optim.AdamW(model.parameters(), lr = 0.001, weight_decay = 0.05)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)
print("Model: ", model)
print("Device: ", DEVICE)

epochs = 30
running_loss_history = []
running_correct_history = []
validation_running_loss_history = []
validation_running_correct_history = []

for epoch in range(epochs):

  running_loss = 0.0
  running_correct = 0.0
  validation_running_loss = 0.0
  validation_running_correct = 0.0

  for inputs, labels in training_loader:

    inputs = inputs.to(DEVICE)
    labels = labels.to(DEVICE)
    outputs = model(inputs)
    loss = criterion(outputs, labels)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    _, preds = torch.max(outputs, 1) # 각 행별 최대값 반환

    running_correct += torch.sum(preds == labels.data)
    running_loss += loss.item()

  else:
    # 훈련팔 필요가 없으므로 메모리 절약
    with torch.no_grad(): 
    # gradient 연산을 옵션을 끌 때 사용하는 파이썬 컨텍스트 매니저 ,
    # 이 컨텍스트 내부에서 새로 생성된 텐서들은 requires_grad=False 상태가 되어, 메모리 사용량을 아껴준다. 파이썬의 데코레이터로도 사용할 수 있다

      for val_input, val_label in validation_loader:

        val_input = val_input.to(DEVICE)
        val_label = val_label.to(DEVICE)
        val_outputs = model(val_input)
        val_loss = criterion(val_outputs, val_label)

        _, val_preds = torch.max(val_outputs, 1)
        validation_running_loss += val_loss.item()
        validation_running_correct += torch.sum(val_preds == val_label.data)
    
    scheduler.step()


    epoch_loss = running_loss / len(training_loader)
    epoch_acc = running_correct.float() / len(training_loader)
    running_loss_history.append(epoch_loss)
    running_correct_history.append(epoch_acc)

    val_epoch_loss = validation_running_loss / len(validation_loader)
    val_epoch_acc = validation_running_correct.float() / len(validation_loader)
    validation_running_loss_history.append(val_epoch_loss)
    validation_running_correct_history.append(val_epoch_acc)

    print("===================================================")
    print("epoch: ", epoch + 1)
    print("training loss: {:.5f}, acc: {:5f}".format(epoch_loss, epoch_acc))
    print("validation loss: {:.5f}, acc: {:5f}".format(val_epoch_loss, val_epoch_acc))
    print("lr: ", optimizer.param_groups[0]['lr'])

plt.figure(1)
plt.plot(running_loss_history,'b',label='train_loss')
plt.plot(validation_running_loss_history,'r',label='test_loss')
plt.xlabel('x axis')
plt.ylabel('y axis')
plt.title('loss 비교')
plt.legend(loc='upper right')
plt.show()

acc_history = []
val_acc_history = []
for i in range(len(running_correct_history)):
  acc_history.append(running_correct_history[i].item())

for i in range(len(validation_running_correct_history)):
  val_acc_history.append(validation_running_correct_history[i].item())

plt.figure(1)
plt.plot(acc_history,'b',label='train_acc')
plt.plot(val_acc_history,'r',label='test_acc')
plt.xlabel('x axis')
plt.ylabel('y axis')
plt.title('acc 비교')
plt.legend(loc='upper right')
plt.show()

