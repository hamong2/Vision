# -*- coding: utf-8 -*-
"""VIsionTransformerUnderstanding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16EYjIdyznWHnWcJjubjzGFL9Z6cOJPlr
"""

!pip install einops
!pip install timm

import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import timm

from torch import nn
from torch import Tensor
from torchvision.transforms import Compose, Resize, ToTensor, Normalize
from einops import rearrange, reduce, repeat
from einops.layers.torch import Rearrange, Reduce
from torchsummary import summary

"""# dataset

augmentation 할깡말깡

mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761),
                 padding=None,
                 scale=None, ratio=None,
                 hflip=0.5, vflip=0.0,
                 color_jitter=0.0,
                 auto_augment=None,
                 interpolation='random',
                 re_prob=0.0, re_mode='const', re_count=1, re_num_splits=0,
                 root="./data", download=False
"""

# 데이터 로드
from torchvision import datasets, transforms
import timm.data.transforms_factory as tff

transforms = transforms.Compose([transforms.ToTensor(),
                                 transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
                                 transforms.RandomHorizontalFlip(0.5),
                                 transforms.RandomVerticalFlip(0.0),
                                 transforms.ColorJitter(0.0),
                                 ])

training_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms)
validation_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms)

training_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=96, shuffle=True, num_workers = 4)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=96, shuffle=False, num_workers = 4)

classes = ("plane", "car", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck")

print("trainset 사이즈 : ", len(training_loader.dataset),
"\ntestset 사이즈 : ",len(validation_loader.dataset))

# 이미지 시각화
import numpy as np

def im_convert(tensor):
  image = tensor.clone().detach().numpy()
  image = image.transpose(1, 2, 0)
  image = image * np.array([0.5, 0.5, 0.5] + np.array([0.5, 0.5, 0.5]))
  image = image.clip(0, 1)
  return image

dataiter = iter(training_loader)
images, labels = dataiter.next()
fig = plt.figure(figsize=(10, 4))

for i in np.arange(10):
  # row 2 column 10
  ax = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])
  plt.imshow(im_convert(images[i]))
  ax.set_title(classes[labels[i].item()])

"""# Model inner block

1. ViT
"""

class Patchembedding(nn.Module):
  def __init__(self, in_channels: int=3, patch_size = 4, emb_dim: int=48, img_size: int = 32):
    self.patch_size = patch_size # patch_size는 바꾸어줄 수 있도록 따로 선언 한줄
    super().__init__()
    # image -> (3,224,224)짜리 이미지 -> (196, 768)로 임베딩 
    self.projection = nn.Sequential(
        nn.Conv2d(in_channels, emb_dim, kernel_size = patch_size, stride = patch_size),
        Rearrange("b d h w -> b (h w) d")
        ) # output = [batch, patch_num, emb_dim] 형태
    self.cls_token = nn.Parameter(torch.randn(1,1, emb_dim))
    # 197 -> 768로 변환한 학습가능한 1D position// cos, sin positional encoding으로 바꿔보자
    self.position = nn.Parameter(torch.randn((img_size//patch_size)**2+1, emb_dim))  
    #self.cos_position = nn.



  def forward(self, x:Tensor) -> Tensor:
    b = x.shape[0]
    proj_x = self.projection(x)
    cls_token = repeat(self.cls_token, '() c d -> b c d', b = b)
    proj_x = torch.concat([cls_token, proj_x], dim=1)
    position = self.position
    proj_x = proj_x + position
    return proj_x

class MultiHeadAttention(nn.Module):
  def __init__(self, emb_dim = 48, num_heads = 4, dropout:float=0.):
    self.emb_dim = emb_dim
    self.num_heads = num_heads
    super().__init__()
    self.querry = nn.Linear(emb_dim, emb_dim)
    self.key = nn.Linear(emb_dim, emb_dim)
    self.value = nn.Linear(emb_dim, emb_dim)
    self.projection = nn.Linear(emb_dim, emb_dim)
    self.drop = nn.Dropout(dropout)

# dropout/ Mask 사용 연구

  def forward(self, token: Tensor) -> Tensor:
    queries = rearrange(self.querry(token), "b n (h d) -> b h n d", h = self.num_heads)
    keys = rearrange(self.key(token), "b n (h d) -> b h n d", h = self.num_heads)
    values  = rearrange(self.value(token), "b n (h d) -> b h n d", h = self.num_heads)
    energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)

    attention = F.softmax(energy, dim=-1)/(self.emb_dim**(1/2))
    #attention = self.drop(attention)
    att_score = torch.einsum('bhqk, bhvd -> bhqd', attention, values)
    result = rearrange(att_score, 'b h n d -> b n (h d)')
    result = self.projection(result) # MLP층 

    return result

class ResidualAdd(nn.Module):
    def __init__(self, fn):
        super().__init__()
        self.fn = fn
        
    def forward(self, x, **kwargs):
        res = x
        x = self.fn(x, **kwargs)
        x += res
        return x

class FeedForwardBlock(nn.Sequential):
    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):
        super().__init__(
            nn.Linear(emb_size, expansion * emb_size),
            nn.GELU(),
            nn.Dropout(drop_p),
            nn.Linear(expansion * emb_size, emb_size),
        )

class TransformerBlock(nn.Sequential):
    def __init__(self,
                 emb_size: int = 48,
                 dropout: float = 0.,
                 forward_expansion: int = 4,
                 forward_dropout: float = 0.,
                 ** kwargs):
        super().__init__(
            ResidualAdd(nn.Sequential(
                nn.LayerNorm(emb_size),
                MultiHeadAttention(emb_size, **kwargs),
                nn.Dropout(dropout)
            )),
            ResidualAdd(nn.Sequential(
                nn.LayerNorm(emb_size),
                FeedForwardBlock(
                    emb_size, expansion=forward_expansion, drop_p=forward_dropout),
                nn.Dropout(dropout)
            )
            ))

class TransformerEncoder(nn.Sequential):
    def __init__(self, 
                 depth: int = 12, 
                 emb_dim: int = 48,
                 dropout: float = 0.,
                 forward_expansion: int = 4,
                 forward_dropout: float = 0.,
                 ** kwargs):
        super().__init__(*[TransformerBlock(emb_dim, dropout, forward_expansion, forward_dropout, **kwargs) for _ in range(depth)])

class Classifier(nn.Sequential):
    def __init__(self, emb_size: int = 48, n_classes: int = 10):
        super().__init__(
            Reduce('b n e -> b e', reduction='mean'),
            nn.LayerNorm(emb_size), 
            nn.Linear(emb_size, n_classes))

class ViT(nn.Sequential):

    def __init__(self, 
                 img_size: int = 32, patch_size: int = 2, num_classes: int = 10, depth: int = 8, num_heads: int=3, emb_dim: int=48,
                 forward_expansion: int =4, in_channel=3, dim_head=64, dropout=0.0, forward_dropout=0.0,
                 embedding=None, classifier=None,
                 name="vit", **kwargs):
        super().__init__()
        self.name = name
        self.embedding = Patchembedding(in_channel, patch_size, emb_dim, img_size) if embedding is None else embedding
        self.transformers = TransformerEncoder(depth, emb_dim, dropout, forward_expansion, forward_dropout)
        self.classifier = Classifier(emb_dim, num_classes) if classifier is None else classifier

    def forward(self, x):
        x = self.embedding(x)
        x = self.transformers(x)
        x = self.classifier(x)

        return x

ex_tensor = torch.randn(8,3,32,32)
ex_tensor.shape

model = ViT()

model(ex_tensor).shape

"""2. ResNet"""



"""3. AlterNet"""



"""# 학습

"""

import torch.optim as optim

USE_CUDA = torch.cuda.is_available()
DEVICE = torch.device("cuda" if USE_CUDA else "cpu")
model = ViT().to(DEVICE)
criterion = F.cross_entropy
optimizer = optim.AdamW(model.parameters(), lr = 0.000125, weight_decay = 0.05)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0)
print("Model: ", model)
print("Device: ", DEVICE)

epochs = 100
running_loss_history = []
running_correct_history = []
validation_running_loss_history = []
validation_running_correct_history = []

for epoch in range(epochs):

  running_loss = 0.0
  running_correct = 0.0
  validation_running_loss = 0.0
  validation_running_correct = 0.0

  for inputs, labels in training_loader:

    inputs = inputs.to(DEVICE)
    labels = labels.to(DEVICE)
    outputs = model(inputs)
    loss = criterion(outputs, labels)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    _, preds = torch.max(outputs, 1) # 각 행별 최대값 반환

    running_correct += torch.sum(preds == labels.data)
    running_loss += loss.item()

  else:
    # 훈련팔 필요가 없으므로 메모리 절약
    with torch.no_grad(): 
    # gradient 연산을 옵션을 끌 때 사용하는 파이썬 컨텍스트 매니저 ,
    # 이 컨텍스트 내부에서 새로 생성된 텐서들은 requires_grad=False 상태가 되어, 메모리 사용량을 아껴준다. 파이썬의 데코레이터로도 사용할 수 있다

      for val_input, val_label in validation_loader:

        val_input = val_input.to(DEVICE)
        val_label = val_label.to(DEVICE)
        val_outputs = model(val_input)
        val_loss = criterion(val_outputs, val_label)

        _, val_preds = torch.max(val_outputs, 1)
        validation_running_loss += val_loss.item()
        validation_running_correct += torch.sum(val_preds == val_label.data)
    
    scheduler.step()


    epoch_loss = running_loss / len(training_loader)
    epoch_acc = running_correct.float() / len(training_loader)
    running_loss_history.append(epoch_loss)
    running_correct_history.append(epoch_acc)

    val_epoch_loss = validation_running_loss / len(validation_loader)
    val_epoch_acc = validation_running_correct.float() / len(validation_loader)
    validation_running_loss_history.append(val_epoch_loss)
    validation_running_correct_history.append(val_epoch_acc)

    print("===================================================")
    print("epoch: ", epoch + 1)
    print("training loss: {:.5f}, acc: {:5f}".format(epoch_loss, epoch_acc))
    print("validation loss: {:.5f}, acc: {:5f}".format(val_epoch_loss, val_epoch_acc))
    print("lr: ", optimizer.param_groups[0]['lr'])

"""# 시각화"""

acc_history = []
val_acc_history = []
for i in range(len(running_correct_history)):
  acc_history.append(running_correct_history[i].item())

for i in range(len(validation_running_correct_history)):
  val_acc_history.append(validation_running_correct_history[i].item())

plt.figure(1)
plt.plot(running_loss_history,'b',label='train_loss')
plt.plot(validation_running_loss_history,'r',label='test_loss')
plt.xlabel('x axis')
plt.ylabel('y axis')
plt.title('loss 비교')
plt.legend(loc='upper right')
plt.show()

plt.figure(1)
plt.plot(acc_history,'b',label='train_acc')
plt.plot(val_acc_history,'r',label='test_acc')
plt.xlabel('x axis')
plt.ylabel('y axis')
plt.title('acc 비교')
plt.legend(loc='upper right')
plt.show()

