# -*- coding: utf-8 -*-
"""ViT_experiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rWdlpwM5P6OTpTggUozS-pNKGNGX9rUB
"""

!pip install einops

import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt

from torch import nn
from torch import Tensor
from torchvision.transforms import Compose, Resize, ToTensor, Normalize
from einops import rearrange, reduce, repeat
from einops.layers.torch import Rearrange, Reduce
from torchsummary import summary

ex_tensor = torch.randn(8,3,32,32)
ex_tensor.shape

"""# Patch Embedding"""

class Patchembedding(nn.Module):
  def __init__(self, in_channels: int=3, patch_size = 2, emb_dim: int=12, img_size: int = 32):
    self.patch_size = patch_size # patch_size는 바꾸어줄 수 있도록 따로 선언 한줄
    super().__init__()
    # image -> (3,224,224)짜리 이미지 -> (196, 768)로 임베딩 
    self.projection = nn.Sequential(
        nn.Conv2d(in_channels, emb_dim, kernel_size = patch_size, stride = patch_size),
        Rearrange("b d h w -> b (h w) d")
        ) # output = [batch, patch_num, emb_dim] 형태
    self.cls_token = nn.Parameter(torch.randn(1,1, emb_dim))
    # 197 -> 768로 변환한 학습가능한 1D position// cos, sin positional encoding으로 바꿔보자
    self.position = nn.Parameter(torch.randn((img_size//patch_size)**2+1, emb_dim))  
    #self.cos_position = nn.



  def forward(self, x:Tensor) -> Tensor:
    b = x.shape[0]
    proj_x = self.projection(x)
    cls_token = repeat(self.cls_token, '() c d -> b c d', b = b)
    proj_x = torch.concat([cls_token, proj_x], dim=1)
    position = self.position
    proj_x = proj_x + position
    return proj_x

Patch_embedded = Patchembedding()(ex_tensor)
Patch_embedded.shape

"""# Multihead Attention"""

class MultiHeadAttention(nn.Module):
  def __init__(self, emb_dim = 12, num_heads = 4, dropout:float=0.):
    self.emb_dim = emb_dim
    self.num_heads = num_heads
    super().__init__()
    self.querry = nn.Linear(emb_dim, emb_dim)
    self.key = nn.Linear(emb_dim, emb_dim)
    self.value = nn.Linear(emb_dim, emb_dim)
    self.projection = nn.Linear(emb_dim, emb_dim)
    self.drop = nn.Dropout(dropout)

# dropout/ Mask 사용 연구

  def forward(self, token: Tensor) -> Tensor:
    queries = rearrange(self.querry(token), "b n (h d) -> b h n d", h = self.num_heads)
    keys = rearrange(self.key(token), "b n (h d) -> b h n d", h = self.num_heads)
    values  = rearrange(self.value(token), "b n (h d) -> b h n d", h = self.num_heads)
    energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)

    attention = F.softmax(energy, dim=-1)/(self.emb_dim**(1/2))
    #attention = self.drop(attention)
    att_score = torch.einsum('bhqk, bhvd -> bhqd', attention, values)
    result = rearrange(att_score, 'b h n d -> b n (h d)')
    result = self.projection(result) # MLP층 

    return result

MultiHeadAttention()(Patch_embedded).shape

"""Residual Block/ Feed forward network

"""

class ResidualAdd(nn.Module):
    def __init__(self, fn):
        super().__init__()
        self.fn = fn
        
    def forward(self, x, **kwargs):
        res = x
        x = self.fn(x, **kwargs)
        x += res
        return x

class FeedForwardBlock(nn.Sequential):
    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):
        super().__init__(
            nn.Linear(emb_size, expansion * emb_size),
            nn.GELU(),
            nn.Dropout(drop_p),
            nn.Linear(expansion * emb_size, emb_size),
        )

class TransformerEncoderBlock(nn.Sequential):
    def __init__(self,
                 emb_size: int = 12,
                 drop_p: float = 0.,
                 forward_expansion: int = 4,
                 forward_drop_p: float = 0.,
                 ** kwargs):
        super().__init__(
            ResidualAdd(nn.Sequential(
                nn.LayerNorm(emb_size),
                MultiHeadAttention(emb_size, **kwargs),
                nn.Dropout(drop_p)
            )),
            ResidualAdd(nn.Sequential(
                nn.LayerNorm(emb_size),
                FeedForwardBlock(
                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),
                nn.Dropout(drop_p)
            )
            ))

TransformerEncoderBlock()(Patch_embedded).shape

class TransformerEncoder(nn.Sequential):
    def __init__(self, depth: int = 16, **kwargs):
        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])

class ClassificationHead(nn.Sequential):
    def __init__(self, emb_size: int = 12, n_classes: int = 10):
        super().__init__(
            Reduce('b n e -> b e', reduction='mean'),
            nn.LayerNorm(emb_size), 
            nn.Linear(emb_size, n_classes))

class ViT(nn.Sequential):
    def __init__(self,     
                in_channels: int = 3,
                patch_size: int = 4,
                emb_size: int = 12,
                img_size: int = 32,
                depth: int = 16,
                n_classes: int = 10,
                **kwargs):
        super().__init__(
            Patchembedding(in_channels, patch_size, emb_size, img_size),
            TransformerEncoder(depth, emb_size=emb_size, **kwargs),
            ClassificationHead(emb_size, n_classes)
        )

"""기존 데이터셋을 이용해 Transfer learning된 모델 이용 -> 성능평가"""

model = ViT()
#model.load_state_dict(torch.load('model_weight.pth'))
model.eval()

"""ImageNet을 이용한 사전학습

"""

import torchvision
transform = Compose(
    [ToTensor(),
     Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

batch_size = 4

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

dataiter = iter(trainloader)
images, labels = dataiter.next()
images.shape

import matplotlib.pyplot as plt
import numpy as np

# 이미지를 보여주기 위한 함수

batch_size = 4
def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()


# 학습용 이미지를 무작위로 가져오기
dataiter = iter(trainloader)
images, labels = dataiter.next()

# 이미지 보여주기
imshow(torchvision.utils.make_grid(images))
# 정답(label) 출력
print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))

import torch.optim as optim 
criterion = nn.CrossEntropyLoss() 
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
#optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(10):
  running_loss = 0.0
  for i,data in enumerate(trainloader, 0):
    inputs, labels = data
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

    running_loss += loss.item()
    if i % 2000 == 1999:
      print('[%d %5d] loss: %.3f' %(epoch+1, i+1, running_loss/2000))
      running_loss = 0.0
PATH = './cifar_net.pth'
torch.save(model.state_dict(), PATH)

dataiter = iter(testloader)
images, labels = dataiter.next()

# 이미지를 출력합니다.
imshow(torchvision.utils.make_grid(images))
print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))

outputs = model(images)
_, predicted = torch.max(outputs, 1)

print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'
                              for j in range(4)))

model = ViT()
model.load_state_dict(torch.load(PATH))

correct = 0
total = 0
# 학습 중이 아니므로, 출력에 대한 변화도를 계산할 필요가 없습니다
with torch.no_grad():
    for data in testloader:
        images, labels = data
        # 신경망에 이미지를 통과시켜 출력을 계산합니다
        outputs = model(images)
        # 가장 높은 값(energy)를 갖는 분류(class)를 정답으로 선택하겠습니다
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')



"""# Experiment

# 1. 훈련단계에서의 조정 
1. 이미지 데이터셋 augmentation
CIFAR dataset augmentation
shear, flip, rotation, affine, colorjitter
"""



import torchvision
from torchvision import transforms 

transform_train = transforms.Compose([transforms.Resize((32, 32)),
                                      transforms.RandomHorizontalFlip(),
                                      transforms.RandomRotation(10),
                                      transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),
                                      transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
                                      transforms.ToTensor(),
                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
                                     ])

transformer = transforms.Compose([transforms.Resize((32, 32)),
                                  transforms.ToTensor(),
                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
                                 ])


training_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transformer)

trainloader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=4, shuffle=True)
testloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=4, shuffle=False)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

"""2. Optimizer 두 가지 비교 (Adam/ SGD) -> 파라미터 조정"""



"""# 2. Transformer Architecture 자체의 수정

1. Transformer의 블록 개수 조정 : CIFAR10 데이터셋이 작기에 Low pass filter역할의 transformer block 개수가 많아질수록 성능이 떨어진다는 가정
"""



"""2. 위와 같은 가정으로, 현재 한 patch size를 4x4로 하였는데 resolution을 좀 더 향상시키기 위하여 patch size를 2로 하여서 진행."""



"""3. Hybird module을 사용하는 개념으로 CIFAR 각 patch를 resnet을 통과시킨 후 transformer block에 넣는 방식 이용.

   CNN(HPF) -> MSA block 성능 향상될 것이라는 가정
"""

